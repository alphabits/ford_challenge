\BOOKMARK [-1][]{part.1}{I Introduction and data exploration}{}
\BOOKMARK [0][]{chapter.1}{1 Introduction}{part.1}
\BOOKMARK [1][]{section.1.1}{1.1 The competition}{chapter.1}
\BOOKMARK [2][]{subsection.1.1.1}{1.1.1 Problemformulering}{section.1.1}
\BOOKMARK [0][]{chapter.2}{2 The Ford Challenge}{part.1}
\BOOKMARK [1][]{section.2.1}{2.1 Other online machine learning competitions}{chapter.2}
\BOOKMARK [2][]{subsection.2.1.1}{2.1.1 Netflix Prize}{section.2.1}
\BOOKMARK [2][]{subsection.2.1.2}{2.1.2 KDD Cup}{section.2.1}
\BOOKMARK [2][]{subsection.2.1.3}{2.1.3 And many others}{section.2.1}
\BOOKMARK [1][]{section.2.2}{2.2 Kaggle.com and The Ford Challenge}{chapter.2}
\BOOKMARK [2][]{subsection.2.2.1}{2.2.1 The data set}{section.2.2}
\BOOKMARK [0][]{chapter.3}{3 Data exploration}{part.1}
\BOOKMARK [1][]{section.3.1}{3.1 A note about Test data, training data}{chapter.3}
\BOOKMARK [1][]{section.3.2}{3.2 Calculating common statistics}{chapter.3}
\BOOKMARK [1][]{section.3.3}{3.3 Determining the datatype of features}{chapter.3}
\BOOKMARK [2][]{subsection.3.3.1}{3.3.1 Unique values}{section.3.3}
\BOOKMARK [2][]{subsection.3.3.2}{3.3.2 Plotting some features}{section.3.3}
\BOOKMARK [1][]{section.3.4}{3.4 Outlier detection}{chapter.3}
\BOOKMARK [2][]{subsection.3.4.1}{3.4.1 Making boxplots of features}{section.3.4}
\BOOKMARK [2][]{subsection.3.4.2}{3.4.2 Making layered feature plots}{section.3.4}
\BOOKMARK [2][]{subsection.3.4.3}{3.4.3 Features that are constant in a trial}{section.3.4}
\BOOKMARK [1][]{section.3.5}{3.5 Finding possible discriminating features}{chapter.3}
\BOOKMARK [2][]{subsection.3.5.1}{3.5.1 Testing binary features}{section.3.5}
\BOOKMARK [2][]{subsection.3.5.2}{3.5.2 Scatterplots}{section.3.5}
\BOOKMARK [1][]{section.3.6}{3.6 Principal Component Analysis}{chapter.3}
\BOOKMARK [2][]{subsection.3.6.1}{3.6.1 Theory}{section.3.6}
\BOOKMARK [2][]{subsection.3.6.2}{3.6.2 Result}{section.3.6}
\BOOKMARK [1][]{section.3.7}{3.7 Conclusions}{chapter.3}
\BOOKMARK [-1][]{part.2}{II Modelling}{}
\BOOKMARK [0][]{chapter.4}{4 Theory of classification}{part.2}
\BOOKMARK [1][]{section.4.1}{4.1 The binary classification problem}{chapter.4}
\BOOKMARK [1][]{section.4.2}{4.2 Parametric models and maximum likelihood}{chapter.4}
\BOOKMARK [2][]{subsection.4.2.1}{4.2.1 Maximum likelihood}{section.4.2}
\BOOKMARK [1][]{section.4.3}{4.3 Logistic Regression}{chapter.4}
\BOOKMARK [2][]{subsection.4.3.1}{4.3.1 The decision boundary}{section.4.3}
\BOOKMARK [2][]{subsection.4.3.2}{4.3.2 Model complexity}{section.4.3}
\BOOKMARK [1][]{section.4.4}{4.4 Feed Forward Neural Network}{chapter.4}
\BOOKMARK [2][]{subsection.4.4.1}{4.4.1 Perceptron}{section.4.4}
\BOOKMARK [2][]{subsection.4.4.2}{4.4.2 Model complexity}{section.4.4}
\BOOKMARK [1][]{section.4.5}{4.5 Measuring classifier performance}{chapter.4}
\BOOKMARK [2][]{subsection.4.5.1}{4.5.1 Cross validation}{section.4.5}
\BOOKMARK [3][]{subsubsection.4.5.1.1}{4.5.1.1 K-fold cross validation}{subsection.4.5.1}
\BOOKMARK [3][]{subsubsection.4.5.1.2}{4.5.1.2 Leave-one-out cross validation}{subsection.4.5.1}
\BOOKMARK [2][]{subsection.4.5.2}{4.5.2 Managing the dataset}{section.4.5}
\BOOKMARK [1][]{section.4.6}{4.6 Model selection}{chapter.4}
\BOOKMARK [2][]{subsection.4.6.1}{4.6.1 Forward feature selection}{section.4.6}
\BOOKMARK [2][]{subsection.4.6.2}{4.6.2 Regularization}{section.4.6}
\BOOKMARK [0][]{chapter.5}{5 Recreating winning approach}{part.2}
\BOOKMARK [1][]{section.5.1}{5.1 The winning approach}{chapter.5}
\BOOKMARK [2][]{subsection.5.1.1}{5.1.1 Classification method}{section.5.1}
\BOOKMARK [1][]{section.5.2}{5.2 Recreating the winning approach}{chapter.5}
\BOOKMARK [2][]{subsection.5.2.1}{5.2.1 Statistics on the AUC score}{section.5.2}
\BOOKMARK [0][]{chapter.6}{6 Improving the winning approach}{part.2}
\BOOKMARK [1][]{section.6.1}{6.1 Forward selection}{chapter.6}
\BOOKMARK [2][]{subsection.6.1.1}{6.1.1 Testing performance on the original testset}{section.6.1}
\BOOKMARK [3][]{subsubsection.6.1.1.1}{6.1.1.1 Statistics on the AUC-score}{subsection.6.1.1}
\BOOKMARK [2][]{subsection.6.1.2}{6.1.2 Testing performance on the top 3 features}{section.6.1}
\BOOKMARK [3][]{subsubsection.6.1.2.1}{6.1.2.1 Statistics on the AUC-score}{subsection.6.1.2}
\BOOKMARK [0][]{chapter.7}{7 Other classification methods}{part.2}
\BOOKMARK [-1][]{part.3}{III Workflow and discussion}{}
\BOOKMARK [0][]{chapter.8}{8 Workflow and tools}{part.3}
\BOOKMARK [0][]{chapter.9}{9 Discussion}{part.3}
\BOOKMARK [0][]{chapter.10}{10 Conclusion}{part.3}
\BOOKMARK [0][]{chapter.11}{A Appendices}{part.3}
\BOOKMARK [1][]{section.11.1}{A.1 Calculating common statistics - Code}{chapter.11}
\BOOKMARK [1][]{section.11.2}{A.2 Calculating common statistics - Result}{chapter.11}
\BOOKMARK [1][]{section.11.3}{A.3 Calculate unique values of features - Code}{chapter.11}
\BOOKMARK [1][]{section.11.4}{A.4 Calculate unique values of features - Result}{chapter.11}
\BOOKMARK [1][]{section.11.5}{A.5 Creating boxplots - Code}{chapter.11}
\BOOKMARK [1][]{section.11.6}{A.6 Creating boxplots - Result}{chapter.11}
\BOOKMARK [1][]{section.11.7}{A.7 Creating layered featuer plots - Code}{chapter.11}
\BOOKMARK [1][]{section.11.8}{A.8 Creating layered feature plots - Result}{chapter.11}
\BOOKMARK [1][]{section.11.9}{A.9 Scatterplots - Code}{chapter.11}
\BOOKMARK [1][]{section.11.10}{A.10 Scatterplots - Result}{chapter.11}
\BOOKMARK [1][]{section.11.11}{A.11 Recreating the winning approach - Code}{chapter.11}
\BOOKMARK [1][]{section.11.12}{A.12 Forward selection - Code}{chapter.11}
\BOOKMARK [1][]{section.11.13}{A.13 Forward selection - Results}{chapter.11}
