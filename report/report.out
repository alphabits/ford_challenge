\BOOKMARK [0][]{chapter.1}{1 Introduction}{}
\BOOKMARK [1][]{section.1.1}{1.1 The competition}{chapter.1}
\BOOKMARK [2][]{subsection.1.1.1}{1.1.1 Problemformulering}{section.1.1}
\BOOKMARK [0][]{chapter.2}{2 The Ford Challenge}{}
\BOOKMARK [1][]{section.2.1}{2.1 Other online machine learning competitions}{chapter.2}
\BOOKMARK [2][]{subsection.2.1.1}{2.1.1 The Netflix Prize}{section.2.1}
\BOOKMARK [2][]{subsection.2.1.2}{2.1.2 KDD Cup}{section.2.1}
\BOOKMARK [2][]{subsection.2.1.3}{2.1.3 And many others}{section.2.1}
\BOOKMARK [1][]{section.2.2}{2.2 Kaggle.com and The Ford Challenge}{chapter.2}
\BOOKMARK [2][]{subsection.2.2.1}{2.2.1 The dataset}{section.2.2}
\BOOKMARK [0][]{chapter.3}{3 Data exploration}{}
\BOOKMARK [1][]{section.3.1}{3.1 Overview of the datasets}{chapter.3}
\BOOKMARK [1][]{section.3.2}{3.2 Calculating common statistics}{chapter.3}
\BOOKMARK [1][]{section.3.3}{3.3 Determining the datatype of features}{chapter.3}
\BOOKMARK [2][]{subsection.3.3.1}{3.3.1 Unique values}{section.3.3}
\BOOKMARK [2][]{subsection.3.3.2}{3.3.2 Plotting some features}{section.3.3}
\BOOKMARK [1][]{section.3.4}{3.4 Outlier detection}{chapter.3}
\BOOKMARK [2][]{subsection.3.4.1}{3.4.1 Making boxplots of features}{section.3.4}
\BOOKMARK [2][]{subsection.3.4.2}{3.4.2 Making layered feature plots}{section.3.4}
\BOOKMARK [2][]{subsection.3.4.3}{3.4.3 Features that are constant in a trial}{section.3.4}
\BOOKMARK [1][]{section.3.5}{3.5 Finding possible discriminating features}{chapter.3}
\BOOKMARK [2][]{subsection.3.5.1}{3.5.1 Scatterplots}{section.3.5}
\BOOKMARK [1][]{section.3.6}{3.6 Principal Component Analysis}{chapter.3}
\BOOKMARK [2][]{subsection.3.6.1}{3.6.1 Theory}{section.3.6}
\BOOKMARK [2][]{subsection.3.6.2}{3.6.2 Result}{section.3.6}
\BOOKMARK [0][]{chapter.4}{4 Theory of classification}{}
\BOOKMARK [1][]{section.4.1}{4.1 The binary classification problem}{chapter.4}
\BOOKMARK [1][]{section.4.2}{4.2 Parametric models and maximum likelihood}{chapter.4}
\BOOKMARK [2][]{subsection.4.2.1}{4.2.1 Maximum likelihood}{section.4.2}
\BOOKMARK [1][]{section.4.3}{4.3 Logistic Regression}{chapter.4}
\BOOKMARK [2][]{subsection.4.3.1}{4.3.1 The decision boundary}{section.4.3}
\BOOKMARK [2][]{subsection.4.3.2}{4.3.2 Model complexity}{section.4.3}
\BOOKMARK [1][]{section.4.4}{4.4 The Neural Network}{chapter.4}
\BOOKMARK [2][]{subsection.4.4.1}{4.4.1 Perceptron}{section.4.4}
\BOOKMARK [2][]{subsection.4.4.2}{4.4.2 Neural Network for binary classification}{section.4.4}
\BOOKMARK [2][]{subsection.4.4.3}{4.4.3 Model complexity}{section.4.4}
\BOOKMARK [1][]{section.4.5}{4.5 Measuring classifier performance}{chapter.4}
\BOOKMARK [2][]{subsection.4.5.1}{4.5.1 AUC}{section.4.5}
\BOOKMARK [3][]{subsubsection.4.5.1.1}{4.5.1.1 Generating the ROC curve}{subsection.4.5.1}
\BOOKMARK [3][]{subsubsection.4.5.1.2}{4.5.1.2 Good and bad AUC scores}{subsection.4.5.1}
\BOOKMARK [2][]{subsection.4.5.2}{4.5.2 Critiques of AUC}{section.4.5}
\BOOKMARK [2][]{subsection.4.5.3}{4.5.3 Cross validation}{section.4.5}
\BOOKMARK [3][]{subsubsection.4.5.3.1}{4.5.3.1 K-fold cross validation}{subsection.4.5.3}
\BOOKMARK [2][]{subsection.4.5.4}{4.5.4 Confidence interval of estimated performance}{section.4.5}
\BOOKMARK [1][]{section.4.6}{4.6 Model selection}{chapter.4}
\BOOKMARK [2][]{subsection.4.6.1}{4.6.1 Forward feature selection}{section.4.6}
\BOOKMARK [0][]{chapter.5}{5 Recreating winning approach}{}
\BOOKMARK [1][]{section.5.1}{5.1 The winning approach}{chapter.5}
\BOOKMARK [2][]{subsection.5.1.1}{5.1.1 Classification method}{section.5.1}
\BOOKMARK [1][]{section.5.2}{5.2 Recreating the winning approach}{chapter.5}
\BOOKMARK [2][]{subsection.5.2.1}{5.2.1 Statistics on the AUC score}{section.5.2}
\BOOKMARK [0][]{chapter.6}{6 Improving the winning approach}{}
\BOOKMARK [1][]{section.6.1}{6.1 Forward selection}{chapter.6}
\BOOKMARK [2][]{subsection.6.1.1}{6.1.1 Testing performance on the original testset}{section.6.1}
\BOOKMARK [3][]{subsubsection.6.1.1.1}{6.1.1.1 Statistics on the AUC-score}{subsection.6.1.1}
\BOOKMARK [2][]{subsection.6.1.2}{6.1.2 Testing performance on the top 3 features}{section.6.1}
\BOOKMARK [3][]{subsubsection.6.1.2.1}{6.1.2.1 Statistics on the AUC-score}{subsection.6.1.2}
\BOOKMARK [0][]{chapter.7}{7 Neural Network}{}
\BOOKMARK [1][]{section.7.1}{7.1 The winning model features}{chapter.7}
\BOOKMARK [2][]{subsection.7.1.1}{7.1.1 3 hidden units - 8 iterations}{section.7.1}
\BOOKMARK [3][]{subsubsection.7.1.1.1}{7.1.1.1 Statistics on the AUC-score}{subsection.7.1.1}
\BOOKMARK [2][]{subsection.7.1.2}{7.1.2 5 hidden units - 8 iterations}{section.7.1}
\BOOKMARK [3][]{subsubsection.7.1.2.1}{7.1.2.1 Statistics on the AUC-score}{subsection.7.1.2}
\BOOKMARK [2][]{subsection.7.1.3}{7.1.3 3 hidden units - 20 iterations}{section.7.1}
\BOOKMARK [3][]{subsubsection.7.1.3.1}{7.1.3.1 Statistics on the AUC-score}{subsection.7.1.3}
\BOOKMARK [2][]{subsection.7.1.4}{7.1.4 5 hidden units - 20 iterations}{section.7.1}
\BOOKMARK [3][]{subsubsection.7.1.4.1}{7.1.4.1 Statistics on the AUC-score}{subsection.7.1.4}
\BOOKMARK [1][]{section.7.2}{7.2 The forward selection features}{chapter.7}
\BOOKMARK [2][]{subsection.7.2.1}{7.2.1 3 hidden units - 8 iterations}{section.7.2}
\BOOKMARK [3][]{subsubsection.7.2.1.1}{7.2.1.1 Statistics on the AUC-score}{subsection.7.2.1}
\BOOKMARK [2][]{subsection.7.2.2}{7.2.2 5 hidden units - 8 iterations}{section.7.2}
\BOOKMARK [3][]{subsubsection.7.2.2.1}{7.2.2.1 Statistics on the AUC-score}{subsection.7.2.2}
\BOOKMARK [2][]{subsection.7.2.3}{7.2.3 forward-selection - 3 hidden units - 20 iterations}{section.7.2}
\BOOKMARK [3][]{subsubsection.7.2.3.1}{7.2.3.1 Statistics on the AUC-score}{subsection.7.2.3}
\BOOKMARK [2][]{subsection.7.2.4}{7.2.4 forward-selection - 5 hidden units - 20 iterations}{section.7.2}
\BOOKMARK [3][]{subsubsection.7.2.4.1}{7.2.4.1 Statistics on the AUC-score}{subsection.7.2.4}
\BOOKMARK [0][]{chapter.8}{8 Discussion}{}
\BOOKMARK [0][]{chapter.9}{9 Conclusion}{}
\BOOKMARK [0][]{chapter.10}{A Workflow, tools and project evaluation}{}
\BOOKMARK [1][]{section.10.1}{A.1 Workflow}{chapter.10}
\BOOKMARK [2][]{subsection.10.1.1}{A.1.1 The session concept}{section.10.1}
\BOOKMARK [2][]{subsection.10.1.2}{A.1.2 Documentation with Sphinx}{section.10.1}
\BOOKMARK [2][]{subsection.10.1.3}{A.1.3 How it worked in the real world}{section.10.1}
\BOOKMARK [1][]{section.10.2}{A.2 Version control}{chapter.10}
\BOOKMARK [1][]{section.10.3}{A.3 NumPy/SciPy, scikits.learn and pybrain}{chapter.10}
\BOOKMARK [2][]{subsection.10.3.1}{A.3.1 Comments on the choice of Python instead of Matlab}{section.10.3}
\BOOKMARK [1][]{section.10.4}{A.4 General reflections on the project}{chapter.10}
\BOOKMARK [0][]{chapter.11}{B Appendices}{}
\BOOKMARK [1][]{section.11.1}{B.1 Calculating common statistics - Code}{chapter.11}
\BOOKMARK [1][]{section.11.2}{B.2 Calculating common statistics - Result}{chapter.11}
\BOOKMARK [1][]{section.11.3}{B.3 Calculate unique values of features - Code}{chapter.11}
\BOOKMARK [1][]{section.11.4}{B.4 Calculate unique values of features - Result}{chapter.11}
\BOOKMARK [1][]{section.11.5}{B.5 Creating boxplots - Code}{chapter.11}
\BOOKMARK [1][]{section.11.6}{B.6 Creating boxplots - Result}{chapter.11}
\BOOKMARK [1][]{section.11.7}{B.7 Creating layered featuer plots - Code}{chapter.11}
\BOOKMARK [1][]{section.11.8}{B.8 Creating layered feature plots - Result}{chapter.11}
\BOOKMARK [1][]{section.11.9}{B.9 Scatterplots - Code}{chapter.11}
\BOOKMARK [1][]{section.11.10}{B.10 Scatterplots - Result}{chapter.11}
\BOOKMARK [1][]{section.11.11}{B.11 Recreating the winning approach - Code}{chapter.11}
\BOOKMARK [1][]{section.11.12}{B.12 Forward selection - Code}{chapter.11}
\BOOKMARK [1][]{section.11.13}{B.13 Forward selection - Results}{chapter.11}
