\chapter{Theory of classification}
In this chapter some of the theory of classification is explained. The discussion is constrained to binary classification, of which the Ford Challenge classifier, is an example. First a general problem definition is given and notation is introduced. Then three different approaches to classification are introduced, and some pros and cons of each approach are mentioned. After this two concrete examples of classification models are introduced (Logistic Regression and Feed Forward Neural Network), and they are related to the three general approaches of classification. Finally different ways to measure the performance of different classifiers are discussed, including AUC that was used as the grading method in The Ford Challenge.

\section[The binary classification problem]{The binary classification problem\protect\footnote{This section is loosely based on \citet[sec 22.1-22.2]{wasserman04}}}
In a binary classification problem a binary outcome $t$ must be predicted from a $d$-dimensional input vector $\ve{x}=[x_1, x_2, \dots, x_d]^T$. The input vector represents an event, and the outcome variable $t$ represents the assignment of the event to one of two classes.
\begin{Exa}
    In \TFC\ the input is an instant in a driving situation, and this input should be assigned to either the class alert, or the class not-alert. The input is represented by an 30-dimensional vector and the assignment to a class is represented by $t=0$ meaning not-alert and $t=1$ alert.
\end{Exa}
To make the prediction possible, a trainingset $\mathcal{T}$ is given consisting of $n$ pairs of input vectors and corresponding, known outcomes.
\[
    \mathcal{T} = \bigl\{\:(\ve{x}_i, t_i)\:\bigr\}
\]
Based on this trainingset, a classification rule $f$ is learned, that can predict the outcome, of yet unknown input vectors. Therefore a classification rule is a function $f\,:\,\R^d\to\{0,1\}$, that is used to make the prediction $t=f(x)$ on a new input. \par
A common way to obtain a classification rule is by the Bayes classification rule. First define $p_k(\ve{x})$ as
    \[
        p_k(\ve{x}) = P(t=k|\ve{x}),\quad k\in\{0,1\}
    \]
    and then the Bayes classification rule is defined by
    \begin{definition}\label{def:bayes-rule}
        The Bayes classification rule $f^*$ is given by
        \[
            f^*(\ve{x}) = \begin{cases}
                1 & \text{if}\quad p_1(\ve{x})>p_0(\ve{x}) \\
                0 & \text{else}
            \end{cases}
        \]
    \end{definition}
    Since $p_0(\ve{x})=1-p_1(\ve{x})$, the Bayes classification rule can also be written
    \[
        f^*(\ve{x}) = \begin{cases}
            1 & \text{if}\quad p_1(\ve{x}) > \frac{1}{2} \\
            0 & \text{else}
        \end{cases}
    \]
    By the Bayes classification rule we have got a theoretical classification rule. In practice the posterior class probabilities $p_0(\ve{x})$ and $p_1(\ve{x})$ are unknown, so the trainingset must be used to find some approximation for these probabilities. The problem of approximating a probability distribution from a given data set is a classic statistical problem. The next section gives one way to solve this problem.

\section{Parametric models and maximum likelihood}\label{sec:parametric-models-and-likelihood}
Two distinct ways to approximate the probability distribution $\Ptx$ is using either a parametric or a non-paramtric approach. The focus in this report is on the paramtric approach. The distribution $\Ptx$, is therefore approximated by a distribution $\PtxHat$ restricted to a class of distributions
\[
    \mathcal{F} = \Bigl\{\,f(t|\ve{x},\ve{w})\,\Bigr\}
\]
where the distributions $f_{\ve{w}}\in\mathcal{F}$ are uniquely identified by their parameter $\ve{w}=[w_1, w_2, \dots, w_k]^T$. By restricting the approximating distribution $\PtxHat$ to the set $\mathcal{F}$, the problem of approximating $\Ptx$ reduces to the problem of estimating the parameter $\ve{w}$.

\subsection{Maximum likelihood}
There exists some different techniques to estimate the parameter $\ve{w}$ in a parametric model.\footnote{See eg. \citet[Sec.9]{wasserman04}} In this section we look at the most common method for estimation of the parameter in a parametric model, namely the maximum likelihood method.\par
The approximation $\PtxHat$ of the posterior class probability is restricted to the class $\mathcal{F}$. Using the trainingset $\mathcal{T}$, the likelihood function is now defined as the probability of obtaining the data in the trainingset, given as a function of the parameter $\ve{w}$. More formally\footnote{Taken directly from \citet[p.122]{wasserman04}}:
\begin{definition}
    The likelihood function is defined by
    \[
    \mathcal{L}(\ve{w}) = \prod_{i=1}^n f(t_i|\ve{x}_i,\ve{w}),\quad (t_i,\ve{x}_i)\in\mathcal{T}
    \]
    and the log-likelihood function is defined by $\ell(\ve{w})$ = $\log\mathcal{L(\ve{w})}$.
\end{definition}
It is worth noticing that the definition of the likelihood function assumes that the $t_i$'s are independent random variables. An assumption that in the case of \TFC\ may be a bit questionable.\mytodo{Elaborate this point.} \par
Given the definition of the likelihood function, the maximum likelihood estimator is now defined by
\begin{definition}
    The maximum likelihood estimator $\mle$ is the value of $\ve{w}$ that maximizes $\mathcal{L}(\ve{w})$.
\end{definition}
Notice that as the logarithm is a monotonic increasing function, the maximum of $\mathcal{L}(\ve{w})$ is equal to the maximum of $\ell(\ve{w})$. This turns out to be handy, since the log-likelihood often is easier to work with, than the likelihood function. \par
With these definitions it is now time to introduce two concrete examples of classification methods.



\section{Logistic Regression}
In the logistic regression the parametric form of the posterior class probability $P(t=0|\ve{x})$ is assumed to be
\begin{equation}\label{eq:log-def}
    P(t=0|\ve{x}) = \frac{1}{1+e^{-(\Lx)}}
\end{equation}
where $\ve{w}=[w_1, w_2, \dots, w_d]^T$. The expression may seem a bit random, but it comes from the fact that (with $p_0=P(t=0|\ve{x})$)
\begin{align*}
    p_0 &= \frac{1}{1+e^{-(\Lx)}} 
\end{align*}
can be written
\begin{align*}
    p_0 &= \frac{e^{\Lx}}{1+e^{\Lx}} \myimp \\
    p_0 + p_0e^{\Lx} &= e^{\Lx} \myimp \\
    \frac{p_0}{1-p_0} &= e^{\Lx} \myimp \\
    \log \frac{p_0}{1-p_0} &= \Lx
\end{align*}
And therefore the parametric form in \eqref{eq:log-def}, comes from the assumptions that the log-odds of $p_0$ is linear in the input vector $\ve{x}$. \par
Now to find an expression for the likelihood function, the probability of getting the data in the trainingset must be expressed. With $p_i=P(t=0|\ve{x}_i)$ the likelihood function can be written as\mytodo{Mention the Bernoulli distribution}
\[
    \mathcal{L}(\ve{w}) = \prod_{i=1}^n p_i^{t_i}(1-p_i)^{1-t_i}
\]
which gives the log-likelihood function
\begin{equation}\label{eq:log-like-logistic}
    \ell(\ve{w}) = \sum_{i=1}^n t_i\log p_i + (1-t_i)\log(1-p_i)
\end{equation}
The maximum of the log-likelihood function gives the maximum likelihood estimator of $\ve{w}$, so the derivative of $\ell$ wrt. $\ve{w}$ need to be derived. Using the fact that
\[
    \frac{\partial p_i(\ve{w})}{\partial \ve{w}} = p_i(1-p_i)\ve{x}_i
\]
gives that
\begin{align*}
    \frac{\partial \ell(\ve{w})}{\ve{w}} &= \sum_{i=1}^n t_i \frac{1}{p_i}p_i(1-p_i)\ve{x}_i + (1-t_i)\frac{1}{1-p_i}(-p_i)(1-p_i)\ve{x}_i \\
    &= \sum_{i=1}^n t_i(1-p_i)\ve{x}_i - (1-t_i)p_i\ve{x}_i \\
    &= \sum_{i=1}^n (t_i-p_i)\ve{x}_i
\end{align*}
The maximum likelihood estimator can now be found by solving $\frac{\partial\ell(\ve{w})}{\partial\ve{w}}=0$, but since $p_i$ isn't linear wrt. $\ve{w}$ the equation must be solved numerically. An effective algorithm called iterative reweighted least squares can solve this problem \citep[p.207]{bishop}.\mytodo{Show that the Hessian is Negative Semi Definite and therefore an unique maximum exists}

\subsection{The decision boundary}
An important property of a classification method is how complicated data sets the method is able to separate in the appropriate classes. The decision boundary of a classification method gives some imformation about how well it separates input data. The decision boundary is the set of points in the input space that lays at the boundary between input points that are classified as class 0 and input points that classify as class 1.
\begin{Exa}
    A trainingset consists of $n$ $d$-dimensional input data $\ve{x}_i=[x_{1i}, x_{2i}, \dots, x_{di}]^T$ and corresponding classes $t_i$. Using linear regression, approximations of the posterior class probabilities $\widehat{p}_k(\ve{x})=\widehat{P}(t=k|\ve{x}), k\in\{0,1\}$ are given. Using these with the Bayes classification rule gives
    \[
        f(\ve{x}) = \begin{cases}
            1 & \text{if}\quad \widehat{p}_1(\ve{x})>\widehat{p}_0(\ve{x}) \\
            0 & \text{else}
        \end{cases}
    \]
    The decision boundary is given by all $\ve{x}$ where $\widehat{p}_0(\ve{x})=\widehat{p}_1(\ve{x})$. But by the definition of the linear regression
    \[
        \log\frac{\widehat{p}_1(\ve{x})}{\widehat{p}_0(\ve{x})} = \Lx
    \]
    So for all points at the decision boundary the following holds
    \[
        \Lx = 0
    \]
    This shows that the decision boundary of the logistic regression is linear in the inputs.
\end{Exa}
The classification methods with linear decision boundaries, defines a family of classification methods with simple analytical and computational properties \citep[p.179]{bishop}. This simplicity comes at the cost of the models having a limited capability to separate complex data sets. 

\subsection{Model complexity}\label{sec:logistic-complexity}
Although the logistic regression has a limited complexity, its ability to separate complex datasets can be increased by introducing feature transformations. Instead of just fitting the logistic regression to the $d$-dimensional input $\ve{x}=[x_1, x_2, \dots, x_d]$, the squared transformations of the elements $x_i$ are added to the input. This gives the $2d$-dimensional input 
\[
    \ve{x}_e = [x_1, \dots, x_d, x_1^2, \dots, x_d^2]
\]
This gives nonlinear decision boundaries in the original input space, which increases the number of datasets that can be separated. Notice that the decision boundary is still linear in the extended $2d$-dimensional inputspace. Instead of introducing transformed, non-linear features in the logistic regression, other methods exists, that have the non-linearity build-in. One example of such a model is the Neural Network.


\section{Feed Forward Neural Network}\label{sec:feed-forward-neural-network}
The Neural Network originated from an attempt to create a mathematical model of information processing in the human brain \citep[p.226]{bishop}. But the Neural Network turned out to be useful in many pratical applications, and it is now seen as a standard statistical model \citep[p.392]{hastie09}. \par
A neural network is made up of one or more layers and each layer consists of one or more perceptrons. The perceptron is the basic building block of a neural network and it is described next

\subsection{Perceptron}
A perceptron consists of one or more inputs $\ve{x}$ with corresponding weights $\ve{w}$ and an activation function $f$. The activation function is assumed to be differentiable. The output from the perceptron is then
\[
    y = f(\ve{w}^T\ve{x} + w_0)
\]
The intercept value $w_0$ is added to make the model more flexible. It can be seen as the weight of an extra input that is always 1.
As a stand alone unit, the perceptron can implement linear classification methods, as eg. logistic regression. To emulate the behavior of a logistic regression, the activation function is chosen as
\[
    f(x) = \frac{1}{1+e^{-x}}
\]
Now the output of the perceptron is modelled as the posterior class probability $P(t=0|\ve{x})$, which gives
\[
    P(t=0|\ve{x}) = \frac{1}{1+e^{-(\ve{w}^T\ve{x}+w_0)}}
\]
If this expression is compared with equation \eqref{eq:log-def} it is seen that the two are identical. An error function can now be defined exactly as the log-likelihood in the logistic regression (see equation \eqref{eq:log-like-logistic}) and a maximum weight vector can be found. \par
The perceptron by itself isn't much more complex than the logistic regression, and the decision boundary is still linear in the input space. The non-linearity is achieved when more than one perceptron is combined in a network.

\subsection{Model complexity}



\section{Measuring classifier performance}\label{sec:classifier-performance}
When a classifier has been trained on a training dataset, some estimate of the classifier performance, on future datasets, needs to be estimated. To be able to estimate the performance, it first needs to be defined exactly how the performance is measured. This can be done in many ways. For the Ford Challenge a measure called AUC-score was used and this measure will be explained in the following section. But even though a measure has been chosen there still needs to be found a way to estimate the expected measure-score on future datasets, for the classifier. An emperical way to do this, is called cross validation.

\subsection{Cross validation}
One straight forward way to get an estimate of the expected performance on future datasets for a classification method, would be to measure the classifiers performance on the dataset used to train the classifier. The problem is that this will give an overly optimistic estimate, since the classifier has been trained to take into account some of the inevitable noise in the trainingset. To get a better estimate of the classifier performance, the complete dataset is split into smaller dataset, such that the same dataset is never used for both training and estimation of performance. This is the core idea in cross validation. Split the dataset to avoid using the same dataset for training and estimation. \par
The easiest way to split a dataset is to split it in two parts: a trainingset and a validationset. The classifier can then be trained on the traningset and its performance is estimated on the validationset. Although this technique works it use the dataset only once, and only one estimate of the performance is produced. This mean that the variability in the estimate can't be estimated. A more effective way to split a dataset is a technique called $K$-fold cross validation

\subsubsection{$K$-fold cross validation}
In $K$-fold cross validation the dataset is split into $K$-parts. Then each part is selected as validationset once and all other parts are the trainingset. The classifier is then trained on $\frac{K-1}{K}$ of the dataset and validated on the last $\frac{1}{K}$ of the dataset. This way $K$ estimates of the classifier performance is calculated, and these can be used to both estimate the expected classifier performance and the variablility in this estimate.

\subsubsection{Leave-one-out cross validation}
This method is just a special case of $K$-fold where $K$ is selected as the same as the total number of rows in the dataset. This means that each row in the dataset is used once as validationset and all the other rows than the current validationrow is used for training. For a dataset of size $n$, this gives $n$ estimates of the data and almost all the data is used for training in each iteration. The problem is that the method is computational demanding isnce the classifier must be trained $n$ times on a dataset of size $n-1$.


\subsection{Managing the dataset}
When starting out a new classification project a dataset is provided. Typically this dataset is split into two parts
\begin{itemize}
    \item {\bfseries Trainingset} - This dataset is used in the rest of the data modelling phase. At any moment in the data modelling phase where different calssification model must be compared the trainingset will be split in smaller trainingset(s) and a validationset. Possibly by $K$-fold cross validation.
    \item {\bfseries Publicationset} - This dataset is kept isolated from the data modelling phase, and is only used when the final classification method has been chosen. At this moment the performance of the final classification method is estimated, by splitting the publication dataset in a trainingset and a validationset, and then measuring performance of this validationset. The splitting can be done by $K$-fold cross validation to get $K$ estimates of the final performance.
\end{itemize}


\section{Model selection}
As mentioned in the section about logistic regression and neural networks, the complexity of a model can be tweaked in different ways. A natural question is how complex the model should be chosen. Should the complexity be as high as is computational possible? Or should the complexity be kept as low as possible? As is often the case, the answer is that the complexity should be somewhere between low and high and it is due to the concept of over- and underfitting. \par
If the model complexity is made is high as is computational possible, it will probably suffer from what is called overfitting. This means that the model has learned so many details from the training data, that it has adapted to the noise in the training data. Although this gives a great performance on the trainingset, it also means that the model will perform worse on future datasets. That is, the generalization of the model gets worse, when the fit to the trainingdata gets to high. \par
At the opposite end of the scale is a model that is too simple. When a model is to simple it can't adapt to the patterns in the training dataset, that is shared with the future dataset. That way the model will not fit neither the trainingset nor any future datasets. \par
One way to determine the right level of complexity is by using cross validation. By training and then estimating performance of models of different complexity levels, the complexity level that achieves the highest performance can be selected.

\subsection{Forward feature selection}
One way to tweak model complexity, that is independent of the classification model used, is feature selection. In feature selection a subset of the whole set of features is selected for the classification model. Since the performance can depend on combinations of features, all possible subset of features should ideally be tested to find the optimal subset. For any high-dimensional input space this is not possible.
\begin{Exa}
    For the Ford Challenge the input data is 30-dimensional. This gives $2^30\approx10^9$ unique subsets to test. Since the model must be trained for each unique subset this is not a feasible solution.
\end{Exa}
Instead of trying every combination of features, differen heuristics exists, that find a solution that is better than many alternatives. But optimallity isn't guaranteed. One such heuristic is the forward feature selection. The procedure is simple. Start with a model that consists of no features and a set of candidate features that could be included in the model. Add the first candidate to the model and test performance. Then exchange the first candidate with the next and test performance, etc. The candidate that gives the best performance is removed from the candidate set and permanently added to the model. Recursively apply this procedure until the performance is no longer improved by adding features. \par
The forward feature selection is a greedy algorithm that at each stage selects the feature that gives the best performance. This do not guarantee an optimal performance but the worst case algorithmic complexity is (with $d$ features in the candidate set)
\[
    \sum_{i=1}^d i = \frac{d(d-1)}{2}
\]
The algorithmic complexity is therefore reduced from an exponential $O(2^d)$ to a polynomial $O(d^2)$.

\subsection{Regularization}
