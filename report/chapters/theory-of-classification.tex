\chapter{Theory of classification}
In this chapter some of the theory of classification is explained. The discussion is constrained to binary classification, of which the Ford Challenge classifier, is an example. First a general problem definition is given and notation is introduced. Then three different approaches to classification are introduced, and some pros and cons of each approach are mentioned. After this two concrete examples of classification models are introduced (Logistic Regression and Feed Forward Neural Network), and they are related to the three general approaches of classification. Finally different ways to measure the performance of different classifiers are discussed, including AUC that was used as the grading method in The Ford Challenge.

\section[The binary classification problem]{The binary classification problem\protect\footnote{This section is loosely based on \citet[sec 22.1-22.2]{wasserman04}}}
In a binary classification problem a binary outcome $t$ must be predicted from a $d$-dimensional input vector $\ve{x}=[x_1, x_2, \dots, x_d]^T$. The input vector represents an event, and the outcome variable $t$ represents the assignment of the event to one of two classes.
\begin{Exa}
    In \TFC\ the input is an instant in a driving situation, and this input should be assigned to either the class alert, or the class not-alert. The input is represented by an 30-dimensional vector and the assignment to a class is represented by $t=0$ meaning not-alert and $t=1$ alert.
\end{Exa}
To make the prediction possible, a trainingset $\mathcal{T}$ is given consisting of $n$ pairs of input vectors and corresponding, known outcomes.
\[
    \mathcal{T} = \bigl\{\:(\ve{x}_i, t_i)\:\bigr\}
\]
Based on this trainingset, a classification rule $f$ is learned, that can predict the outcome, of yet unknown input vectors. Therefore a classification rule is a function $f\,:\,\R^d\to\{0,1\}$, that is used to make the prediction $t=f(x)$ on a new input. \par
A common way to obtain a classification rule is by the Bayes classification rule. First define $p_k(\ve{x})$ as
    \[
        p_k(\ve{x}) = P(t=k|\ve{x}),\quad k\in\{0,1\}
    \]
    and then the Bayes classification rule is defined by
    \begin{definition}\label{def:bayes-rule}
        Let $\alpha\in[0,\infty[$. The Bayes classification rule $f^*$ is given by
        \[
            f^*(\ve{x}) = \begin{cases}
                1 & \text{if}\quad \alpha p_1(\ve{x})>p_0(\ve{x}) \\
                0 & \text{else}
            \end{cases}
        \]
    \end{definition}
    Since $p_0(\ve{x})=1-p_1(\ve{x})$, the Bayes classification rule can be written
    \[
        f^*(\ve{x}) = \begin{cases}
            1 & \text{if}\quad p_1(\ve{x}) > \frac{1}{1+\alpha} \\
            0 & \text{else}
        \end{cases}
    \]
    By the Bayes classification rule we have got a theoretical classification rule. In practice the posterior class probabilities $p_0(\ve{x})$ and $p_1(\ve{x})$ are unknown, so the trainingset must be used to find some approximation for these probabilities. The problem of approximating a probability distribution from a given data set is a classic statistical problem. The next section gives one way to solve this problem.

\section{Parametric models and maximum likelihood}\label{sec:parametric-models-and-likelihood}
Two distinct ways to approximate the probability distribution $\Ptx$ is using either a parametric or a non-paramtric approach. The focus in this report is on a paramtric approach. The distribution $\Ptx$, is therefore approximated by a distribution $\PtxHat$ restricted to a class of distributions
\[
    \mathcal{F} = \Bigl\{\,f(t|\ve{x},\ve{w})\,\Bigr\}
\]
where the distributions $f_{\ve{w}}\in\mathcal{F}$ are uniquely identified by their parameter $\ve{w}=[w_1, w_2, \dots, w_k]^T$. By restricting the approximating distribution $\PtxHat$ to the set $\mathcal{F}$, the problem of approximating $\Ptx$ reduces to the problem of estimating the parameter $\ve{w}$.

\subsection{Maximum likelihood}
There exists some different techniques to estimate the parameter $\ve{w}$ in a parametric model.\footnote{See eg. \citet[Sec.9]{wasserman04}} In this section we look at the most common method for estimation of the parameter in a parametric model, namely the maximum likelihood method.\par
The approximation $\PtxHat$ of the posterior class probability is restricted to the class $\mathcal{F}$. Using the trainingset $\mathcal{T}$, the likelihood function is now defined as the probability of obtaining the data in the trainingset, given as a function of the parameter $\ve{w}$. More formally\footnote{Taken directly from \citet[p.122]{wasserman04}}:
\begin{definition}
    The likelihood function is defined by
    \[
    \mathcal{L}(\ve{w}) = \prod_{i=1}^n f(t_i|\ve{x}_i,\ve{w}),\quad (t_i,\ve{x}_i)\in\mathcal{T}
    \]
    and the log-likelihood function is defined by $\ell(\ve{w})$ = $\log\mathcal{L(\ve{w})}$.
\end{definition}
It is worth noticing that the definition of the likelihood function assumes that the $t_i$'s are independent random variables. An assumption that in the case of \TFC\ may be a bit questionable. \par
Given the definition of the likelihood function, the maximum likelihood estimator is now defined by
\begin{definition}
    The maximum likelihood estimator $\mle$ is the value of $\ve{w}$ that maximizes $\mathcal{L}(\ve{w})$.
\end{definition}
Notice that as the logarithm is a monotonic increasing function, the maximum of $\mathcal{L}(\ve{w})$ is equal to the maximum of $\ell(\ve{w})$. This turns out to be handy, since the log-likelihood often is easier to work with, than the likelihood function. \par
With these definitions it is now time to introduce two concrete examples of classification methods.



\section{Logistic Regression}
In the logistic regression the parametric form of the posterior class probability $P(t=0|\ve{x})$ is assumed to be
\begin{equation}\label{eq:log-def}
    P(t=0|\ve{x}) = \frac{1}{1+e^{-(\Lx)}}
\end{equation}
where $\ve{w}=[w_1, w_2, \dots, w_d]^T$. The expression may seem a bit random, but it comes from the fact that (with $p_0=P(t=0|\ve{x})$)
\begin{align*}
    p_0 &= \frac{1}{1+e^{-(\Lx)}} 
\end{align*}
can be written
\begin{align*}
    p_0 &= \frac{e^{\Lx}}{1+e^{\Lx}} \myimp \\
    p_0 + p_0e^{\Lx} &= e^{\Lx} \myimp \\
    \frac{p_0}{1-p_0} &= e^{\Lx} \myimp \\
    \log \frac{p_0}{1-p_0} &= \Lx
\end{align*}
And therefore the parametric form in \eqref{eq:log-def}, comes from the assumptions that the log-odds of $p_0$ is linear in the input vector $\ve{x}$. \par
Now to find an expression for the likelihood function, the probability of getting the data in the trainingset must be expressed. With $p_i=P(t=0|\ve{x}_i)$ the likelihood function can be written as
\[
    \mathcal{L}(\ve{w}) = \prod_{i=1}^n p_i^{t_i}(1-p_i)^{1-t_i}
\]
which gives the log-likelihood function
\[
    \ell(\ve{w}) = \sum_{i=1}^n t_i\log p_i + (1-t_i)\log(1-p_i)
\]
The maximum of the log-likelihood function gives the maximum likelihood estimator of $\ve{w}$, so the derivative of $\ell$ wrt. $\ve{w}$ need to be derived. Using the fact that
\[
    \frac{\partial p_i(\ve{w})}{\partial \ve{w}} = p_i(1-p_i)\ve{x}_i
\]
gives that
\begin{align*}
    \frac{\partial \ell(\ve{w})}{\ve{w}} &= \sum_{i=1}^n t_i \frac{1}{p_i}p_i(1-p_i)\ve{x}_i + (1-t_i)\frac{1}{1-p_i}(-p_i)(1-p_i)\ve{x}_i \\
    &= \sum_{i=1}^n t_i(1-p_i)\ve{x}_i - (1-t_i)p_i\ve{x}_i \\
    &= \sum_{i=1}^n (t_i-p_i)\ve{x}_i
\end{align*}
The maximum likelihood estimator can now be found by solving $\frac{\partial\ell(\ve{w})}{\partial\ve{w}}=0$, but since $p_i$ isn't linear wrt. $\ve{w}$ the equation must be solved numerically. An effective algorithm called iterative reweighted least squares can solve this problem \citep[p.207]{bishop}.

\subsection{The decision boundary}
An important property of a classification method is how complicated data sets the method is able to separate in the appropriate classes. The decision boundary of a classification method gives some imformation about how well it separates input data. The decision boundary is the set of points in the input space that lays at the boundary between input points that are classified as class 0 and input points that classify as class 1.
\begin{Exa}
    A trainingset consists of $n$ $d$-dimensional input data $\ve{x}_i=[x_{1i}, x_{2i}, \dots, x_{di}]^T$ and corresponding classes $t_i$. Using linear regression, approximations of the posterior class probabilities $\widehat{p}_k(\ve{x})=\widehat{P}(t=k|\ve{x}), k\in\{0,1\}$ are given. Using these with the Bayes classification rule gives
    \[
        f(\ve{x}) = \begin{cases}
            1 & \text{if}\quad \widehat{p}_1(\ve{x})>\widehat{p}_0(\ve{x}) \\
            0 & \text{else}
        \end{cases}
    \]
    The decision boundary is given by all $\ve{x}$ where $\widehat{p}_0(\ve{x})=\widehat{p}_1(\ve{x})$. But by the definition of the linear regression
    \[
        \log\frac{\widehat{p}_1(\ve{x})}{\widehat{p}_0(\ve{x})} = \Lx
    \]
    So for all points at the decision boundary the following holds
    \[
        \Lx = 0
    \]
    This shows that the decision boundary of the logistic regression is linear in the inputs.
\end{Exa}
The classification methods with linear decision boundaries, defines a family of classification methods with simple analytical and computational properties \citep[p.179]{bishop}. This simplicity comes at the cost of the models having a limited capability to separate complex data sets. 

\subsection{Model complexity}\label{sec:logistic-complexity}
Although the logistic regression has a limited complexity, its ability to separate complex datasets can be increased by introducing feature transformations. Instead of just fitting the logistic regression to the $d$-dimensional input $\ve{x}=[x_1, x_2, \dots, x_d]$, the squared transformations of the elements $x_i$ are added to the input. This gives the $2d$-dimensional input 
\[
    \ve{x}_e = [x_1, \dots, x_d, x_1^2, \dots, x_d^2]
\]
This gives nonlinear decision boundaries in the original input space, which increases the number of datasets that can be separated. Notice that the decision boundary is still linear in the extended $2d$-dimensional inputspace. Also there still exists datasets that can't be separated by a linear classifier, no matter what feature transformations are introduced\mytodo{Morten: Is this correct? Where can I find a theorem stating this?}

\subsection{Regularization}


\section{Feed Forward Neural Network}\label{sec:feed-forward-neural-network}
The Neural Network originated from an attempt to create a mathematical model of information processing in the human brain \citep[p.226]{bishop}. But the Neural Network turned out to be useful in many pratical applications, and it is now seen as a standard statistical model \citep[p.392]{hastie09}.
\subsection{Model complexity}
\subsection{Regularization}



\section{Measuring classifier performance}\label{sec:classifier-performance}
When a classifier has been trained, an estimate of the classifiers expected performance on future inputs, should be calculated. To calculate this astimate, it needs to be specified exactly what is meant by a classifiers performance. For that a loss function is defined
\begin{definition}
    Let $t_0$ be the true class of a corresponding input $\ve{x}_0$, and $f$ be a classification rule. The loss function
    \[
        L(t_0, f(\ve{x}_0))
    \]
    then gives the cost of making the prediction $f(\ve{x})$, when the true class is $t_0$.
\end{definition}
The common way to define the loss function is by
\begin{equation}\label{eq:loss-function}
    L(t, y) = \begin{cases}
        1 & \text{if}\quad t\neq y \\
        0 & \text{else}
    \end{cases}
\end{equation}
This assumes that the cost of predicting class 0 when the truth is class 1, and visa versa is the same. In many applications this may be questionable. In \TFC\ the cost of predicting alert, when the truth is not-alert, might be a more severe error, than the other way around; although it depends on what the Ford classifier is used for. In the rest of this section, the loss function given in \eqref{eq:loss-function} is assumed. \par
When a loss function has been defined, what would be interesting to know, is the expected loss on future data for a classification rule $f$.
\[
    \Err = \expop [L(t, f(\ve{x}))]
\]
where $t$ and $\ve{x}$ is random variables and the expectation is taken on their joint distribution. The expected loss isn't normally known so it needs to be estimated somehow. For that the average loss over a dataset for a given classification rule can be used. For an arbitrary dataset $\mathcal{D}=\{(\ve{x}_i, t_i)\}, i=1\dots n$, classification rule $f$ and loss function $L$, the average loss is calculated by
\[
    \overline{\text{err}} = \frac{1}{n}\sum_{i=1}^n L(t_i, f(\ve{x}_i))
\]

\subsection{Splitting the dataset}\label{sec:splitting-dataset}
It is extremely important that the dataset used to calculate the average loss, isn't in anyway overlapping with the dataset that was used to train the classifier. If there is a large dataset available for a classification task, the dataset should ideally be split into three separate datasets\footnote{Loosely based on \citet[p.222]{hastie09}}
\begin{itemize}
    \item The traning set $\mathcal{T}$ - This dataset is used to train models. More specifically the training set is used to find the maximum likelihood estimator, for a parametric model
    \item The validation set $\mathcal{V}$ - This dataset is used for model selection.  As mentioned in section~\ref{sec:logistic-complexity}, the complexity of classification methods can be altered to make the method fit the training data better. The validation set gives an estimate of how well models of different complexity levels, predicts future datapoints.
    \item The publication set $\mathcal{P}$ - This dataset is used, when the final model has been selected, and an estimate of the performance of this model. 
\end{itemize}
There is not a general rule for how many percent of the original dataset that should be assigned to each of the three datasets. In \citet[p.222]{hastie09}, a split of 50\% for training and 25\% for bot validation and publication, is suggested.
\begin{Exa}
    In \TFC\ a dataset of 500 trials is given. Following the suggestion by \citet{hastie09}, the dataset is split in a training set of 250 trials and a validation set and publication set of both 125 trials. It is then decided to train a logistic regression model to classify drivers. The complexity of the model is increased by including various feature transformations. The average loss of the different models is calculated on the validation set, and the models with the lowest average loss is selected. Finally the average loss over the publication loss is used as an independent estimate of the expected loss on future data.
\end{Exa}
The above method of making three distinct datasets out of the original, is only possible, when the original dataset is large. And even with a large dataset, it may seem a bit wasteful to only have tested the final model against 50\% of the original dataset\mytodo{Does this make sense?}. A way to circumvent this problem is by using cross validation.

\subsection{Cross validation}
Cross validation is a technique to squeeze out more information of a dataset, than by splitting the dataset in three parts as described in section~\ref{sec:splitting-dataset}\mytodo{Is this right? I have troubles defining cross validation? Is it only a technique for model selection? When assesing performance on test/publication set cross validation could be used?}
Recommendation of 5- or 10-fold cross validation \citep[p.243]{hastie09}
\subsection{AUC}\label{sec:theory:auc}
Mentioning critiques of AUC?


\section{Model selection}
\subsection{Over- and underfitting}
\subsection{Forward feature selection}
\subsection{Regularization}
