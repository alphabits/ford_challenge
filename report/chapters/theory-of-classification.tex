\chapter{Theory of classification}
Here I describe some general results about classification. Bayes error rate. No free lunch theorem. Ugly duckling theorem.

\section{AUC}\label{sec:theory:auc}
Mentioning critiques of AUC?

\section[Principal Component Analysis]{Principal Component Analysis\protect\footnote{Section is based on \citet[p.113]{alpaydin10}}} % (fold)
\label{sec:pca}
The idea in Principal Component Analysis (PCA) is to find a mapping from the $d$-dimensional input space, to a new $k$-dimensional space (with $k<d$), while keeping as much of the variation in the input data, as possible. To begin with let $k=1$. Then the subspace is identified by a $d$-dimensional vector $\ve{w}_1$, that without loss of generality is assumed to have length $\norm{\ve{w}_1}=1$. \par
For any vector $\ve{x}$ in the input space, the projection onto $\ve{w}_1$ is given by
\[
    z_1 = \ve{w}_1^T \ve{x}
\]
and we now wish to maximize the variance of $z_1$. By the definition of variance we get (with $\ve{\mu}=\expop{\ve{x}}$)
\begin{align*}
    \varop{z_1} &= \varop{\ve{w}_1^T\ve{x}} \\
    &= \expop{(\ve{w}_1^T\ve{x} - \ve{w}_1^T\ve{\mu})^2} \\
    &= \expop{(\ve{w}_1^T\ve{x} - \ve{w}_1^T\ve{\mu})(\ve{w}_1^T\ve{x} - \ve{w}_1^T\ve{\mu})} \\
    &= \expop{\ve{w}_1^T(\ve{x}-\ve{\mu})(\ve{x}-\ve{\mu})^T\ve{w}_1} \\
    &= \ve{w}_1^T\expop{(\ve{x}-\ve{\mu})(\ve{x}-\ve{\mu})^T}\ve{w}_1 \\
    &= \ve{w}_1^T\ve{\Sigma}\ve{w}_1
\end{align*}
where $\ve{\Sigma} = \covop{\ve{x}}$. So to maximize the variance of $z_1$ is the same as maximizing $\ve{w}_1^T\ve{\Sigma}\ve{w}_1$, subject to the contraint that $\ve{w}_1^T\ve{w}_1=1$. By introducing a Lagrange multiplier $\alpha$, we get
\[
    \max_{\ve{w}_1} \ve{w}_1^T\ve{\Sigma}\ve{w}_1 - \alpha(\ve{w}_1^T\ve{w}_1 - 1)
\]
This is a normal, unconstrained maximization problem. Taking the derivative with respect to $\ve{w}_1$, and setting this equal to 0, gives
\begin{align*}
    2\ve{\Sigma}\ve{w}_1 - 2\alpha\ve{w}_1 &= 0 \myimp \\
    \ve{\Sigma}\ve{w}_1 &= \alpha\ve{w}_1
\end{align*}
which is true exactly when $\ve{w}_1$ is an eigenvector of $\ve{\Sigma}$ and $\alpha$ is the corresponding eigenvalue.
% section pca (end)
