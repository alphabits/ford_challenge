\chapter{Discussion}

In this chapter some of the discussions from throughout the report will be resumed. \par

Starting with the data exploration, one of the early surprises was the (lack of) data quality. This was hinted at already in the calculation of summary statistics. Some features had datapoints many, many standard deviations from the mean, and some features were simply zero throughout the dataset. Although summary statistics isn't that informative it was a simple way to get started with the data exploration. \par

Calculating the number of unique values within a trial for each feature, was an effective way to get an idea of the datatype of the features. Also the unique values made it clear, that many features had trials where they were constant. \par

Plotting the different features for various trials gave yet another confirmation that this was indeed a real-world dataset. When the time came for outlier detection it was found that it was difficult to make a rational criteria for what data to exclude. It seems to have been the right choice not to remove any data, as later models turned out to depend on two well behaved features \fn{E9} and \fn{V11} as well as two aggregated features \fn{sdE1}, \fn{sdE5}. Had some trials been removed, useful data for these features would have been discarded. \par

Creating scatterplots of all pairs of features didn't really reveal anything, except for the inverse relationship between two pairs of features. The Principal Components Analysis didn't reveal much either except for a possible cluster of not-alert points that were detected. This information could maybe have been used to get better classification. \par

In the data modelling part of the project, the winning model was quickly recreated. It is interesting to note that a competition with 180 participants, is won by using a simple logistic regression on only 3 out of 30 features. Had the features been some ingenious combination of other features, it wouldn't be surprising, but two of the features of the winning model turned up in the simple forward selection done in section~\ref{sec:forward-selection}. Regarding the forward selection, the AUC score was used as the measure for deciding which features to include. This is not the standard way \citep{meetings-morten} of doing a forward selection, but it seems to be a logical choice if the classifier performance in the end is measured by the AUC score. Some confirmation of this argument is seen by the fact, that the forward selection revealed a feature \fn{sdE1} that consistently gave higher AUC score than the feature \fn{sdE5} used in the winning model. The conclusion is weakened a bit by the fact, that the performance of the forward selection model on the real Ford testset is unknown. Perhaps the winner also first chose \fn{sdE1} but then found that \fn{sdE5} performed better on the Ford testset. \par

The attempts to further improve the performance, by training a neural network wasn't that succesful. Most of the results were similar to the results achieved by the logistic regression. Only the run with the forward selected features, 3 hidden layers, and 20 iterations, showed signs of improvements. Had the network been trained for maybe 200 iterations\footnote{An undocumented run with 50 iterations didn't give any improvements though}, further improvements might have been detected. As a final note about the neural network modelling, it was interesting to watch the poorer performance of the network with 5 hidden units, trained for 20 iteration, and using the forward selected features. The performance can be explained by overfitting of the network to the training data, but further experiments should be done before concluding anything. \par

Part of the problem statement was to set up a working environment for data analysis. This was done, and some structure was put on the work process, that helped controlling the chaos that seems inevitable. Also some great libraries for scientific computations and machine learning was found and was put to good use throughout the project.
