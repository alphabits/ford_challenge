\chapter{Neural Network}
In this chapter the logistic regression models from the previous chapters, are tried to be improved. Specifically a neural network is trained on the features of winning model as well as on the top 3 features from the forward selection in section~\ref{sec:forward-selection}. It wasn't possible to train a neural network on all 48 features from the feature selection, due to limited computational resources. Instead it was tested how the number of hidden units as well as the number of iterations of the optimization algorithm, affected the performance. For each model training a random subset of 100000 rows from the trainingset was used.

\section{The winning model features}
In this section the features of winning model are used as input for neural networks with various numbers of hidden units, and trained with a different number of optimization iterations.


\subsection{3 hidden units - 8 iterations}
The basic setting for the network training was a network with 3 hidden units and 8 weight optimization iterations. It is worth noticing that 8 iterationsisn't many iterations, but the trainingset was large (100000 rows) and the computational resources few. \par
A network was trained for the basic settings and the performance was then tested on the testset. The results from the validation are shown in table~\ref{tbl:inference-3-hidden-8-epochs} \par
\begin{table}
    \centering
    {\sffamily\small
        \begin{tabularx}{30mm}{ l R }
            Run & AUC \\\hline
            1 & 0.8132 \\
2 & 0.8074 \\
3 & 0.8116 \\
4 & 0.8040 \\
5 & 0.7959 \\
6 & 0.8070 \\
7 & 0.7975 \\
8 & 0.8046 \\
9 & 0.8035 \\
10 & 0.8006 \\\hline
        \end{tabularx}
    }
    \caption{Results from training a neural network with 3 hidden units for 8 iterations on the features of the winning model}
    \label{tbl:inference-3-hidden-8-epochs}
\end{table}
The results of the neural network is close to the results achieved by the logistic regression in section~\ref{sec:recreating-winner}, although the network seems to be doing a bit worse. A confidence interval is calculated to get an better idea.

\subsubsection{Statistics on the AUC-score}
First the sample mean and sample standard deviation are calculated
\[
    m = 0.8045 \quad\quad\text{and}\quad\quad s = 0.005583
\]
which gives the 95\% confidence interval
\[
    \CI{0.8005}{0.8085}
\]
this is seen to overlap with the confidence interval of the logistic regression calculated in section~\ref{sec:recreating-winner-statistics}


\subsection{5 hidden units - 8 iterations}
The number of hidden units is increased from 3 to 5. The features are still from the winning model and the number of iterations is still 8. Training and then testing the network gives the results seen in table~\ref{tbl:inference-5-hidden-8-epochs}.\par
\begin{table}
    \centering
    {\sffamily\small
        \begin{tabularx}{30mm}{ l R }
            Run & AUC \\\hline
            1 & 0.8060 \\
2 & 0.8087 \\
3 & 0.8048 \\
4 & 0.8032 \\
5 & 0.8068 \\
6 & 0.8025 \\
7 & 0.8046 \\
8 & 0.8060 \\
9 & 0.8070 \\
10 & 0.8020 \\\hline
        \end{tabularx}
    }
    \caption{inference-features - 5 hidden units - 8 iterations}
    \label{tbl:inference-5-hidden-8-epochs}
\end{table} 
No obvious improvements in the AUC score is achieved by raising the number of hidden units. The AUC-score is still a little above 0.8. The usual statistics are calculated on the results.
\subsubsection{Statistics on the AUC-score}
First the sample mean and sample standard deviation are calculated
\[
    m = 0.8052 \quad\quad\text{and}\quad\quad s = 0.002150
\]
which gives the 95\% confidence interval
\[
    \CI{0.8036}{0.8067}
\]
which overlap with the confidence interval of the network with 3 hidden units.


\subsection{3 hidden units - 20 iterations}
Instead of raising the number of hidden units, the number of iterations is increased from 8 to 20. The hidden units are returned to 3. As usual the results are gathered in a table, namely table~\ref{tbl:inference-3-hidden-20-epochs}. \par
\begin{table}
    \centering
    {\sffamily\small
        \begin{tabularx}{30mm}{ l R }
            Run & AUC \\\hline
            1 & 0.7989 \\
2 & 0.8073 \\
3 & 0.7995 \\
4 & 0.8053 \\
5 & 0.8052 \\
6 & 0.8046 \\
7 & 0.8080 \\
8 & 0.8042 \\
9 & 0.8103 \\
10 & 0.8060 \\\hline
        \end{tabularx}
    }
    \caption{inference-features - 3 hidden units - 20 iterations}
    \label{tbl:inference-3-hidden-20-epochs}
\end{table} 
No increase in performance is seen and maybe the number of iterations is simply to low for any effect to be seen. For completeness the usual statistics are calculated.
\subsubsection{Statistics on the AUC-score}
First the sample mean and sample standard deviation are calculated
\[
    m = 0.8049 \quad\quad\text{and}\quad\quad s = 0.003525
\]
which gives the 95\% confidence interval
\[
    \CI{0.8024}{0.8075}
\]



\subsection{5 hidden units - 20 iterations}
For completeness a network was also trained with 5 hidden units and a total of 20 iterations. The results can be seen in table~\ref{tbl:inference-5-hidden-20-epochs}
\begin{table}
    \centering
    {\sffamily\small
        \begin{tabularx}{30mm}{ l R }
            Run & AUC \\\hline
            1 & 0.8010 \\
2 & 0.8017 \\
3 & 0.8072 \\
4 & 0.8033 \\
5 & 0.8027 \\
6 & 0.8131 \\
7 & 0.8064 \\
8 & 0.7991 \\
9 & 0.8050 \\
10 & 0.8029 \\\hline
        \end{tabularx}
    }
    \caption{inference-features - 5 hidden units - 20 iterations}
    \label{tbl:inference-5-hidden-20-epochs}
\end{table} 
Again a consistent performance around the 0.8 is achieved. Not much to declare and the statistics gives no surprises.
\subsubsection{Statistics on the AUC-score}
First the sample mean and sample standard deviation are calculated
\[
    m = 0.8042 \quad\quad\text{and}\quad\quad s = 0.003966
\]
which gives the 95\% confidence interval
\[
    \CI{0.8014}{0.8071}
\]


\section{The forward selection features}
The focus is now turned to the top 3 features selected by forward selection in section~\ref{sec:forward-selection}. The same 4 combinations ($[3,5]\times [8,20]$) of hidden units and number of iterations are tested on the forward selection features. When a logistic regression model was trained on the forward selection features (see section~\ref{sec:top-3-feature-selection}) an increased performance, compared to the winning model features, was observed. It is hoped that the same increase is seen in the neural network models.

\subsection{3 hidden units - 8 iterations}
First a network with 3 hidden units is trained for 8 iterations. The results are shown in table~\ref{tbl:forward-3-hidden-8-epochs}.\par
\begin{table}
    \centering
    {\sffamily\small
        \begin{tabularx}{30mm}{ l R }
            Run & AUC \\\hline
            1 & 0.8286 \\
2 & 0.8336 \\
3 & 0.8345 \\
4 & 0.8354 \\
5 & 0.8272 \\
6 & 0.8344 \\
7 & 0.8370 \\
8 & 0.8339 \\
9 & 0.8385 \\
10 & 0.8320 \\\hline
        \end{tabularx}
    }
    \caption{forward-selection - 3 hidden units - 8 iterations}
\end{table} 
As hoped for the performance is increased compared to the winning models features. Compared to the performance of the logistic regression on the forward selected features no noticeable difference is observed.
\subsubsection{Statistics on the AUC-score}
First the sample mean and sample standard deviation are calculated
\[
    m = 0.8335 \quad\quad\text{and}\quad\quad s = 0.003463
\]
which gives the 95\% confidence interval
\[
    \CI{0.8310}{0.8360}
\]



\subsection{5 hidden units - 8 iterations}
The results for the network model with 5 hidden units trained for 8 iterations are shown in table~\ref{tbl:forward-5-hidden-8-epochs}. \par
\begin{table}
    \centering
    {\sffamily\small
        \begin{tabularx}{30mm}{ l R }
            Run & AUC \\\hline
            1 & 0.8301 \\
2 & 0.8301 \\
3 & 0.8332 \\
4 & 0.8381 \\
5 & 0.8320 \\
6 & 0.8411 \\
7 & 0.8290 \\
8 & 0.8340 \\
9 & 0.8325 \\
10 & 0.8355 \\\hline
        \end{tabularx}
    }
    \caption{forward-selection - 5 hidden units - 8 iterations}
    \label{tbl:forward-5-hidden-8-epochs}
\end{table} 
The performance is more or less identical with the performance for 3 hidden units. It may be due to the fact that 8 iterations are not enough training for the additional complexity to show up. Or maybe there is just not much more structure to be learned in the data.
\subsubsection{Statistics on the AUC-score}
First the sample mean and sample standard deviation are calculated
\[
    m = 0.8336 \quad\quad\text{and}\quad\quad s = 0.003800
\]
which gives the 95\% confidence interval
\[
    \CI{0.8308}{0.8363}
\]


\subsection{forward-selection - 3 hidden units - 20 iterations}
The network returns to 3 hidden unit, but is now trained for 20 iterations. The results are shown in table~\ref{tbl:forward-3-hidden-20-epochs} \par
\begin{table}
    \centering
    {\sffamily\small
        \begin{tabularx}{30mm}{ l R }
            Run & AUC \\\hline
            1 & 0.8373 \\
2 & 0.8432 \\
3 & 0.8443 \\
4 & 0.8395 \\
5 & 0.8382 \\
6 & 0.8441 \\
7 & 0.8377 \\
8 & 0.8382 \\
9 & 0.8431 \\
10 & 0.8285 \\\hline
        \end{tabularx}
    }
    \caption{3 hidden units - 20 iterations}
    \label{tbl:forward-3-hidden-20-epochs}
\end{table} 
From the results it is seen that the performance is increased i little compared to the previous two runs. The difference isn't clear cut though, so a confidence interval may give a clearer picture of how the performances compares.
\subsubsection{Statistics on the AUC-score}
First the sample mean and sample standard deviation are calculated
\[
    m = 0.8394 \quad\quad\text{and}\quad\quad s = 0.004759
\]
which gives the 95\% confidence interval
\[
    \CI{0.8360}{0.8428}
\]
It is seen that the confidence interval just barely overlaps with the confidence intervals from the 2 previous run. It seems as if this network performs slightly better than the previous two networks.


\subsection{forward-selection - 5 hidden units - 20 iterations}
The last neural network test is for a network with 5 hidden units, that are trained for 20 iterations. As a slight increase in performance was observed when the training was run for 20 iterations, similar good performance may be expected from this test. The results are shown in table~\ref{tbl:forward-5-hidden-20-epochs}. \par
\begin{table}
    \centering
    {\sffamily\small
        \begin{tabularx}{30mm}{ l R }
            Run & AUC \\\hline
            1 & 0.8181 \\
2 & 0.8203 \\
3 & 0.8230 \\
4 & 0.8169 \\
5 & 0.8227 \\
6 & 0.8206 \\
7 & 0.8209 \\
8 & 0.8188 \\
9 & 0.8223 \\
10 & 0.8159 \\\hline
        \end{tabularx}
    }
    \caption{5 hidden units - 20 iterations}
    \label{tbl:forward-5-hidden-20-epochs}
\end{table} 
As is seen from the results, the performance have decreased noticeably and is the worst of all the networks trained on the forward selected features. Either the optimization got stuck in a local minimum or the network has been overfitted to the trainingdata. Overfitting could make sense, since this network has more complexity and have had more time to fit this complexity to the trainingset. It is worth noticing though, that no similar decrease in performance was observed in the networks trained on the winning features.
\subsubsection{Statistics on the AUC-score}
First the sample mean and sample standard deviation are calculated
\[
    m = 0.8199 \quad\quad\text{and}\quad\quad s = 0.002463
\]
which gives the 95\% confidence interval
\[
    \CI{0.8182}{0.8217}
\]
The right endpoint of this interval is about 0.01 below the lowest left endpoint of the confidence intervals, for the tests on the networks trained on the feature selected features.

