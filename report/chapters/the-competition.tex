\chapter{The Ford Challenge} 
In this chapter, the ford competition that is the basis for this report, is introduced. Before introducing the ford challenge, a short overview of other online machine learning competitions is given. As part of introducing the ford competition, the kaggle.com website that hosted the ford competition is presented, and the data set used in the ford competition is described in detail. But as metioned the chapter starts with a short overview of other online machine learning competitions.

\section{Other online machine learning competitions}
To get a little perspective on the ford challenge, this section gives a short review of some past and present online machine learning competitions.

\subsection{Netflix Prize}
One of the most talked about competitions, may very well be the Netflix Prize. The Netflix competition was launched on October 2, 2006 and the aim of the competition was to predict how users would grade new movies, based on a large dataset of previous grades. Why did the Netflix Prize gather a lot of attention? First of all the grand prize was \$1M, which is a lot of money. And secondly the dataset was huge, consisting of 100,480,507 ratings given by 480,189 users, leaving room for a lot of interesting new techniques to be tested. \par 
When the Netflix Prize was awarded on September 18, 2009, 5169 different teams had submitted at least one entry to the competition. The winning team consisted of three different teams, that at one point decided to team-up and compete as a join-team.\par 
Netflix originally wished to follow up the Netflix Prize, with another competition but decided to dismiss the idea, due to a lawsuit regarding privacy concerns related to the first Netflix Prize. \footnote{The section about The Netflix Prize is based on \citep{wiki:netflix_prize} and \citep{netflix_leaderboard}}

\subsection{KDD Cup}
Although the Netflix Prize gather a lot attention, it wasn't the first online machine learning competition. An example of an earlier competition, is the KDD Cup. The KDD Cup is a competition that is held every year, and that started back in 1997. The subject changes every year, and can be anything from mining purchase data from an online store, to computer aided detection of breast cancer (the 2000 and 2008 competitions respectively).\par
This year the KDD Cup is held in coorporation with Yahoo! Labs and the task is to predict user ratings of musical items (both tracks, albums, artists and genres). One note worthy detail about the 2011 KDD Cup is the huge data set, containing over 300 million ratings of more than 600,000 distinct items. \footnote{Read more about the KDD Cup history at \citep{kdd_cup_center}. For more info about the 2011 competition see \citep{kdd_cup_2011}}

\subsection{And many others}
The Netflix Prize and the KDD Cup are just two examples of online machine learning competitions. Many others exists, such as
\begin{itemize}
    \item \emph{The Hearst Challenge 2011} - Every year The Hearst Corporation hosts a machine learning competition. This year the task is to data mine the history of 1.8 million emails sent to subscribers of Hearst's publications, and then predict who will open emails in the future. \\
        Read more at \url{http://www.hearstchallenge.com}
    \item \emph{The Reclab Prize} - RichRelevance is a company that specializes in online product recommendation. They offer a \$1M prize for the team that first improves their product recommendation algorithm by 10\%. \\
        Read more at \url{http://www.overstockreclabprize.com}
    \item \emph{The Heritage Health Prize} - 
\end{itemize}
Since almost all the machine learning competition websites, needs the same functionality, websites that specializes in hosting machine learning competitions has appeared. One example of a website that provides a hosting platform for machine learning competitions is the kaggle.com website, who hosts The Ford Challenge.

\section{Kaggle.com and The Ford Challenge}
As already mentioned, kaggle.com hosts machine learning competitions for universities and corporations. The first competition hosted by kaggle.com was started in April 2010, and since then a total of 18 competitions have been held. Every competition at kaggle.com has some background information, links to the data sets, a submission system and a forum, as seen in figure \ref{fig:fordchallenge_frontpage}. \par

\begin{figure}[tbhHp]
    \centering
        \includegraphics[width=.9\textwidth]{media/fordchallenge_frontpage.png}
    \caption{The Ford Challenge frontpage at the kaggle.com website}
    \label{fig:fordchallenge_frontpage}
\end{figure}

The Ford Challenge began on January 19, 2011. The task was to create a classifier that is able to detect when a driver is about to get distracted while driving. The dataset that Ford made available for the competition consisted of measurements of 30 different features, measured on drivers along with a binary feature (IsAlert) that was 1 if the driver was alert and 0 otherwise. The 30 features was a mix of environmental, driver physiological and vehicular features. Based on this dataset a classifier should predict the IsAlert feature of a distinct test dataset held by Ford. \par
One detail that made the competition slightly different than many other competitions, was that Ford would not disclose any information about what the different features represented\footnote{see forum replies from the Ford spokesperson \citep{kaggle_forum_266,kaggle_forum_317}}. The official reason was that (see \citep{kaggle_forum_268_reply_2})
\begin{quote}
    ``We like to encourage the participants to pursue classification without preconceived notions based on prior knowledge of the subject, focusing on variables which lead them (based on their own experiments) to better classification."
\end{quote}
Doubts about the true motive behind the lack of details about the features, was expressed by, what later turned out to be, the winner of the competition (see \citep{kaggle_forum_295_reply_3}) \par

The performance of the classifiers was measured by calculating the AUC (see section \mytodo{AUC-section}) of the classifiers, on the test set. A limit of two submissions pr contestant was set as a way to counteract the possibility of someone reverse-engineering the IsAlert-feature of the test dataset.

\subsection{The data set}


